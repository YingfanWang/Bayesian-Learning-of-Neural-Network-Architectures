# -*- coding: utf-8 -*-
"""
Created on Sat Apr 13 22:20:10 2019

@author: Anton
"""

import torch.nn as nn
import math
from torch.distributions.normal import Normal
from numpy.random import logistic
import torch.nn.functional as F

class BayesLinear(nn.Module):
  
  
  def __init__(self, h1, h2, bypass = True):
    super().__init__()
    self.bypass = bypass
    self.input_size = h1
    self.output_size = h2
    self.weight_mu = nn.Parameter(torch.Tensor(h1, h2).normal_(0,1)) ## Get better init here later
    self.weight_rho = nn.Parameter(torch.Tensor(h1, h2).uniform_(-5,-4))
    self.bias_mu = nn.Parameter(torch.Tensor(h2).normal_(0,0.001))
    self.bias_rho = nn.Parameter(torch.Tensor(h2).uniform_(-5, -4))
    self.layer_mu = nn.Parameter(torch.Tensor(1).normal_(h2 / 2, 0.1)) ## We only have one my parameter for each layer right?
    self.layer_rho = nn.Parameter(torch.Tensor(1).uniform_(-5, -4)) ## They haven't actually written which repara they use for the sigma but I guess it is like usual?
    if bypass:
      self.bypass_a = nn.Parameter(torch.Tensor(1).normal_(0,1))
    self.tau = 1 ## The temperature
    
  
  def truncated_normal(self, x, mu, sigma):
    
    norm = Normal(mu, sigma)
    left_val = norm.cdf(1)
    right_val = norm.cdf(self.output_size)
    
    
    def log_normal(x, mu, sigma):
      return -1 / 2 * torch.log(2*math.pi*sigma**2) - 1/ (2 * sigma**2) * (x-mu)**2
    
    p1 = log_normal(x, mu, sigma)
    p2 = torch.log(right_val - left_val)
    return torch.exp(p1-p2)
  
  ## The parameter pi for the layer bypass gumbal later. bypass_a should be inserted here
  def bypass_pi(self, x):
    return 1 / (1 + torch.exp(-x))
  
  ## Converts rho to sigma parameter
  def sigma(self, x):
    return torch.log1p(torch.exp(-x))
  
  ## This one gives me the probability for the layer vector, later I also have to sample from Gumbal using this as pi
  def layer_pi(self, mu, sigma):
    
    input_x = torch.arange(1,self.output_size + 1).float()
    p1 = self.truncated_normal(input_x, mu, sigma)
    p2 = torch.sum(p1)
    return p1 / p2 ## Maybe do this with a log?
    
  ## I also need some term that calculates all the variational terms right?
  
  def forward(self, x):
    if self.bypass:
      bypass_pi = self.bypass_pi(self.bypass_a)
      random_logistic = torch.tensor(logistic(0,1))
      gamma = 1 / (1 + torch.exp((-(torch.log(bypass_pi)-torch.log(1-bypass_pi) + random_logistic)/self.tau)))

    lay_mu = self.layer_mu
    lay_sig = self.sigma(self.layer_rho)
    lay_pi = self.layer_pi(lay_mu, lay_sig)
    
    ## Need to add the matrix so that has an effect yes
    
    w_mu = self.weight_mu
    w_sigma = self.sigma(self.weight_rho)
    weight = w_mu + w_sigma * torch.Tensor(w_mu.shape).normal_(0,1)
    
    b_mu = self.bias_mu
    b_sigma = self.sigma(self.bias_rho)
    bias = b_mu + b_sigma * torch.Tensor(b_mu.shape).normal_(0,1)
    
    if self.bypass:
      return (1-gamma)*F.linear(x, torch.t(weight), bias) + gamma * x
    else:
      return F.linear(x, torch.t(weight), bias)
    
    
    
    
a = BayesLinear(1,5)
t = torch.Tensor(5,1).normal_(0,1)
a(t)
    
    
    
